\chapter{Radial Basis Function Neural Network}
\label{chapter:rbf}
\section{簡介}
Radial Basis Function Function（簡稱RBF）Network Work，圖\ref{fig:RbfNetwork}爲它的網路架構圖，其中有三層網路，分別爲輸入、隱藏層與輸出層。在介紹這個演算法前我們先來看看圖\ref{fig:PerceptonProblem}，在\ref{fig:PerceptonPa}、\ref{fig:PerceptonPb}情況下，我們都可以利用一條線兩種不同類別分割，但是遇到\ref{fig:PerceptonPc}的情況，卻不能用一條線將兩個類別分割出來，由此可見這就是Single Perceptron缺點。而RBF這個演算法的優勢就在於，可以解決了Single Perceptron在某些情況下線性不可切割的問題，可以將不可線性分割的資料集轉換到另一個維度如圖\ref{Fig:RbfTransfer}所示，使得轉換過的資料變成可線性分割的問題。
\begin{figure}[htbp]
	\centering
	\centerline{\includegraphics[width=10cm]{pic/rbf_struct.png}}
	\caption{RBF網路架構圖}
	\begin{minipage}{.7\linewidth}
		\footnotesize
		\emph{圖片來源:}取自Function approximation using artificial neural networks
	\end{minipage}
	\label{fig:RbfNetwork}
\end{figure}

\begin{figure}[H]
	\begin{center}
		\begin{tabular}{ccccccccccccc}
			\subfigure[]{\includegraphics[height=4cm]{./pic/q9iv7T7i.png}\label{fig:PerceptonPa} } \par &
			\subfigure[]{\includegraphics[height=4cm]{./pic/gQvdNdfq.png}\label{fig:PerceptonPb} } \par &
			\subfigure[]{\includegraphics[height=4cm]{./pic/zLfLBK3B.png}\label{fig:PerceptonPc} } \par   \\
		\end{tabular}
		\caption{}
		\label{fig:PerceptonProblem}
	\end{center}
\end{figure}


%figure3
\begin{figure}[htbp!]
	\centering
	\subfigure[未轉換過的原資料]{
		\includegraphics[width=10cm]{pic/Mxrc4anf.png}
		%\caption{fig1}
	}

	%	\quad
	\subfigure[經過轉換後的資料]{
		\includegraphics[width=10cm]{pic/JmIpjiQ2.png}
	}

	\caption{資料轉換的示意圖}
	\label{Fig:rbf_transfer}
\end{figure}



\section{Radial Basis Function}
前一小節提到RBF神經網路可以將資料轉換到另一個維度，而這個能將資料轉換到另一個維度的函數就是Hiden Layer中的 \(h(.)\)，它也就是Radial Basis Function，以下舉個例子來說明它的運作：

\begin{itemize}
	\item

	      圖\ref{fig:RbfIntroductionNotTransfer}為一個一維的資料集，其中有藍色與橘色兩個類別，而我們的任務就是將這兩個類別分開。乍看之下僅用一條線將其分開，幾乎是不可能的任務，但是這也是我們限制在一個維度下才不能將其分離。



	      \begin{figure}[h]
		      \centering
		      \includegraphics[height=6cm]{./pic/vM4xT9rm.png}
		      \caption{一維資料集}
		      \label{fig:RbfIntroductionNotTransfer}
	      \end{figure}

	\item
	      現在我們有一個 \(f(x)\) ，當我們的資料點經過這個函數的轉換，可以發現橘色點都向上移動，而藍色點都向下移動。經過函數轉換的結果後，我們要將這兩個類別進行切割就不難了。

	      \begin{figure}[h]
		      \centering
		      \includegraphics[width=6cm]{./pic/7VM3Lid5.png}
		      \caption{經過 \(f(x)\)轉換的結果 }
		      \label{fig:RbfWithFunction}
	      \end{figure}


	\item
	      而這個 \(f(x)\)，其實是由3個不同的函數所組成在外加一個常數項所組成的，如圖\ref{fig:RbfFunctionAssemble}與\ref{eqn:RbfFunction}式所示。而其中的 \(h_1(x)\)、\(h_2(x)\)與\(h_3(x)\)就是就是 Radial Basis Function，藉由他們的組成可以將原本的資料集轉換到另一個維度。

	      \begin{equation}
		      \label{eqn:RbfFunction}
		      f(x)=w_1h_1(x)+w_2h_2(x)+w_3h_3(x)+b
	      \end{equation}

	      \begin{figure}[h]
		      \centering
		      \includegraphics[height=6cm]{./pic/vtYD2no6.png}
		      \caption{\(f(x)\)的組成 }
		      \label{fig:RbfFunctionAssemble}
	      \end{figure}

	\item
	      Radial Basis Function（徑向基函數），之所以有個「Radial」，其實不難想像，就是跟中心點的距離有關係，所以通常這個函數會設定一個中心點，並將其他點與這個中心點的距離代入計算輸出結果。
	      式 \ref{eqn:RadialBasisFunction}與圖\ref{fig:RadialBasisFunction}，爲一個中心點為1的高斯函數。
	      從圖中也能發現，離中心點越近可以得到越大值，代表與中心點越相關。反之離中心點越遠所得到的值就越小，代表與中心點越不相關。

	      \begin{equation}
		      \label{eqn:RadialBasisFunction}
		      h(x)= e^{-|1-x|^2}
	      \end{equation}

	      \begin{figure}[H]
		      \centering
		      \includegraphics[height=6cm]{./pic/w0KFLczH.png}
		      \caption{}
		      \label{fig:RadialBasisFunction}
	      \end{figure}

\end{itemize}

當然Rail Basis Function有很多種，常見的如下表所示：
\\

\begin{table}[h!]
	\centering
	\label{tab:rbf_table}
	\input{tables/introduction/rbf_table.tex}
	\caption{常見的Radial Basis Function}
\end{table}




%要被單行註解的文字。


\section{演算法參數定義與流程}




\subsection{參數定義}
\begin{itemize}
	\item
	      輸入向量，\(\mathbf{x} = (x_1,x_2,x_3,....,x_R)\)，爲資料集中的其中一筆資料，輸入向量的 \(R\) 爲資料的特徵數量。
	      %
	\item
	      Radial Basis Layer權重，\(\mathbf{W^1} = [\mathbf{c_1,c_2,...,c_j,...c_{S_1}}]^T\)，爲一個 \(S_1 \times R \)的矩陣，也就是前一節所介紹 Radial Basis Function中心點的集合，這邊的 \(S_1\)代表中心點的數量，而 \(\mathbf{c_j}\)為其中一個中心點 。
	      %
	\item
	      Radial Basis Function， 定義為\(\{h_1(x), h_2(x),...,h_j(x),....h_{S_1}(x) \}\)  。
	      這邊$h_j(x)=\phi(\mathbf{|c_j-x_i|})$。


	\item
	      輸出層權重， \(\mathbf{W^2}= [\mathbf{w_1,w_2,...,w_k,...,w_{S_2}}]^T\)，為一個 \(S_2 \times S_1\) 的矩陣，這邊的 \(S_2\)代表輸出的類別數 。偏移向量為， \(\mathbf{b^2}=(b_1,b_2,....b_{S_2})\)。
	\item
	      輸出向量，\(\mathbf{y}= (y_1,y_2,...,y_k,...y_{S_2})\)，其中式 \ref{eqn:RbfOutputLayer}為每個 \(\mathbf{y_k}\)的計算方式。其實仔細觀察可以發現式\ref{eqn:RbfOutputLayer}與式\ref{eqn:RbfFunction}所代表函意相同。

	      \begin{equation}
		      \label{eqn:RbfOutputLayer}
		      \mathbf{y_k}= f(x) = \sum_{i=1}^{S_1}w_{ik}\cdot h_i(x)+b_k
	      \end{equation}
	      %
	\item
	      實際類別， \(T\)。
\end{itemize}

\subsection{流程}

首先會先初始化 \(\textbf W^1\)  、 \(\textbf W^2\) 與 \(\mathbf{b^2}\) ，其中 \(\mathbf{W^1}\) 的中心點可以利用kmeans演算法得到，則\(\textbf W^2\) 與 \(\mathbf{b^2}\)可隨機生成。接著將所有的資料集進行訓練，利用自行選定的Radial Basis Function計算每筆 \(\mathbf x\)與每個中心點的 \(\{h_1,...,h_{S_1}\}\)，以及計算其輸出結果 \(\mathbf{y}\)，並透過最小平方法更新 \(\mathbf{W^2}\) 與 \(\mathbf{b}\) 的值。直到達到訓練條件。

\begin{figure}[H]
	\centering
	\input{workflow/rbf_workflow.tex}
	\caption{演算法流程圖}
	\label{fig:AlogrithmWorkflow}
\end{figure}

\section {總結}
圖\ref{fig:RbfOutcome}為一個二維資料進行RBF Neural Network 訓練的結果，從圖中可以發現分別有紅藍兩類，黑色的*則代表各個中心點，經過訓練後可以在圖上畫出一個決策邊界。
從這個訓練結果可以明顯的看出RBF Neural Network這個演算法的優點，能順利將這種不可線性分割的資料集進行分類，又因為訓練的參數不多，所以短時間就能訓練出不錯的結果。

此外，在這個演算法中，中心點的數量與位置會影響訓練效果，數量太多會可能會導致訓練結果過擬合，數量太少則會導致訓練的結果不理想，而中心點的位置選的不好，即便數量對了，訓練的效果也可能會不好。




\begin{figure}[h]
	\centering
	\includegraphics[height=5cm]{./pic/v35Qta6w.png}
	\caption{RBF Network以二維資料訓練的示意圖}

	\begin{minipage}{.7\linewidth}
		\centering
		\footnotesize
		\emph{圖片來源:}取自Radial Basis Function Network (RBFN) Tutorial
	\end{minipage}

	\label{fig:RbfOutcome}
\end{figure}
